* Inputs

#+name: blInputs
#+begin_src C++
  double strike = 100.;
  double spot = 100.0;
  double vol = 0.3;
  double rate = 0.03;
  int numPaths = 5000000;
  int numSteps = 300;
  double T = 1.0;
  double dt = T / numSteps;
  double sqrt_dt = std::sqrt(dt);
#+end_src
* Cuda

#+begin_src C++
  #include <iostream>
  #include <curand_kernel.h>

  __global__ void monteCarloSDE(float* d_samples, int numPaths, int numSteps, float dt, float mu, float sigma, float x0) {
      int pathIdx = blockIdx.x * blockDim.x + threadIdx.x;
      if (pathIdx >= numPaths) return;

      // Initialize the state of the random number generator for this thread
      curandState state;
      curand_init(1234, pathIdx, 0, &state);

      // Simulate the SDE for this path
      float x = x0;
      for (int i = 0; i < numSteps; ++i) {
          float dW = curand_normal(&state) * sqrtf(dt); // Wiener increment
          x += mu * dt + sigma * dW; // Euler-Maruyama update
      }

      // Store the final value for this path
      d_samples[pathIdx] = x;
  }


  void mcAnalyticalc(unsigned long startPath, unsigned long numPaths,
                     unsigned long numSteps, double T, double dt,
                     double sqrt_dt, double mu, double sigma, double S0,
                     vector2d &S) {
    std::random_device rd;
    std::mt19937 gen(rd());
    std::normal_distribution<> dis(0.0, 1.0);

    for (unsigned long i = 0; i < numPaths; ++i) {
      S[startPath + i][0] = S0;
      for (unsigned long j = 1; j < numSteps; ++j) {
        double dW = dis(gen) * sqrt_dt;
        S[startPath + i][j] = S[startPath + i][j - 1] *
            std::exp((mu - 0.5 * sigma * sigma) * dt + sigma * dW);
      };
    };
  }


  int main() {

      <<blInputs>>
      
      // Memory allocation (host and device)
      float* h_samples = new float[numPaths];
      float* d_samples;
      cudaMalloc(&d_samples, numPaths * sizeof(float));

      // Define CUDA kernel configuration
      int threadsPerBlock = 256;
      int numBlocks = (numPaths + threadsPerBlock - 1) / threadsPerBlock;

      // Launch kernel
      monteCarloSDE<<<numBlocks, threadsPerBlock>>>(d_samples, numPaths, numSteps, dt, mu, sigma, x0);

      // Copy results back to host
      cudaMemcpy(h_samples, d_samples, numPaths * sizeof(float), cudaMemcpyDeviceToHost);

      // Compute the mean of the simulated final values
      float mean = 0.0f;
      for (int i = 0; i < numPaths; ++i) {
          mean += h_samples[i];
      }
      mean /= numPaths;

      // Output results
      std::cout << "Mean of the final values: " << mean << std::endl;

      // Clean up
      delete[] h_samples;
      cudaFree(d_samples);

      return 0;
  }

#+end_src

** Multiple device

#+begin_src C++
  #include <iostream>
  #include <cuda_runtime.h>
  #include <curand_kernel.h>

  // CUDA kernel to simulate SDE paths using Euler-Maruyama
  __global__ void monteCarloSDE(float* d_results, int numPaths, int numSteps, float dt, float mu, float sigma, float x0, unsigned long seed) {
      int tid = blockIdx.x * blockDim.x + threadIdx.x;
      if (tid >= numPaths) return;

      // Initialize random number generator state
      curandState state;
      curand_init(seed, tid, 0, &state);

      // Simulate SDE using Euler-Maruyama
      float x = x0;
      for (int i = 0; i < numSteps; ++i) {
          float dW = curand_normal(&state) * sqrtf(dt); // Wiener process increment
          x += mu * dt + sigma * dW;                   // Euler-Maruyama update
      }

      // Store the final value for this path
      d_results[tid] = x;
  }

  int main() {
      // Problem configuration
      int numPaths = 1000000;  // Total number of Monte Carlo paths
      int numSteps = 1000;     // Number of time steps
      float dt = 0.001f;       // Time step size
      float mu = 0.1f;         // Drift coefficient
      float sigma = 0.2f;      // Volatility coefficient
      float x0 = 1.0f;         // Initial value

      // Query number of available GPUs
      int numDevices;
      cudaGetDeviceCount(&numDevices);
      if (numDevices < 1) {
          std::cerr << "No CUDA-capable devices found." << std::endl;
          return 1;
      }
      std::cout << "Number of CUDA devices: " << numDevices << std::endl;

      // Divide paths across devices
      int pathsPerDevice = numPaths / numDevices;

      // Results on host
      float* h_results = new float[numPaths];

      // Per-device resources
      float* d_results[numDevices];
      cudaStream_t streams[numDevices];
      for (int i = 0; i < numDevices; ++i) {
          // Set device
          cudaSetDevice(i);

          // Allocate memory for results on GPU
          cudaMalloc(&d_results[i], pathsPerDevice * sizeof(float));

          // Create a CUDA stream for each device
          cudaStreamCreate(&streams[i]);
      }

      // Launch kernels on each device
      for (int i = 0; i < numDevices; ++i) {
          cudaSetDevice(i);

          // Calculate grid and block size
          int threadsPerBlock = 256;
          int numBlocks = (pathsPerDevice + threadsPerBlock - 1) / threadsPerBlock;

          // Launch kernel
          monteCarloSDE<<<numBlocks, threadsPerBlock, 0, streams[i]>>>(
              d_results[i], pathsPerDevice, numSteps, dt, mu, sigma, x0, 1234 + i);

          // Check for kernel errors
          cudaError_t err = cudaGetLastError();
          if (err != cudaSuccess) {
              std::cerr << "CUDA kernel error on device " << i << ": " << cudaGetErrorString(err) << std::endl;
              return 1;
          }
      }

      // Copy results back to host
      for (int i = 0; i < numDevices; ++i) {
          cudaSetDevice(i);
          cudaMemcpyAsync(h_results + i * pathsPerDevice, d_results[i],
                          pathsPerDevice * sizeof(float), cudaMemcpyDeviceToHost, streams[i]);
      }

      // Synchronize all streams
      for (int i = 0; i < numDevices; ++i) {
          cudaSetDevice(i);
          cudaStreamSynchronize(streams[i]);
      }

      // Compute global mean
      double globalSum = 0.0;
      for (int i = 0; i < numPaths; ++i) {
          globalSum += h_results[i];
      }
      double globalMean = globalSum / numPaths;

      // Print the result
      std::cout << "Global mean of Monte Carlo simulation: " << globalMean << std::endl;

      // Clean up
      for (int i = 0; i < numDevices; ++i) {
          cudaSetDevice(i);
          cudaFree(d_results[i]);
          cudaStreamDestroy(streams[i]);
      }
      delete[] h_results;

      return 0;
  }
#+end_src
* Threads
#+begin_src C++ :flags -std=c++20 :noweb yes 
  #include <chrono>
  #include <cmath>
  #include <iostream>
  #include <random>
  #include <stdexcept>
  #include <thread>
  #include <valarray>
  #include <vector>

  using vector2d = std::vector<std::vector<double>>;

  double payoff(unsigned long startPath, unsigned long numPaths, vector2d &S, double strike) {
    double sum = 0.; 
    for (unsigned long i = 0; i < numPaths; ++i) {
      sum += std::max(strike - S[startPath + i].back(), 0.);

      }
    return sum / numPaths;
    }

  void mcAnalyticalc(unsigned long startPath, unsigned long numPaths,
                     unsigned long numSteps, double T, double dt,
                     double sqrt_dt, double mu, double sigma, double S0,
                     vector2d &S) {
    std::random_device rd;
    std::mt19937 gen(rd());
    std::normal_distribution<> dis(0.0, 1.0);

    for (unsigned long i = 0; i < numPaths; ++i) {
      S[startPath + i][0] = S0;
      for (unsigned long j = 1; j < numSteps; ++j) {
        double dW = dis(gen) * sqrt_dt;
        S[startPath + i][j] = S[startPath + i][j - 1] *
            std::exp((mu - 0.5 * sigma * sigma) * dt + sigma * dW);
      };
    };
  }

  void eulerMaruyamac(unsigned long startPath, unsigned long numPaths,
                      unsigned long numSteps, double T, double dt,
                      double sqrt_dt, double mu, double sigma, double S0,
                      vector2d &S) {
    std::random_device rd;
    std::mt19937 gen(rd());
    std::normal_distribution<> dis(0.0, 1.0);

    for (unsigned long i = 0; i < numPaths; ++i) {
      S[startPath + i][0] = S0;
      for (unsigned long j = 1; j < numSteps; ++j) {
        double dW = dis(gen) * sqrt_dt;
        S[startPath + i][j] = mu * S[startPath + i][j - 1] * dt +
                              sigma * S[startPath + i][j - 1] * dW;
      };
    };
  }

  void fthread(double& output, unsigned long startPath, unsigned long numPaths,
                 unsigned long numSteps, double T, double dt,
                 double sqrt_dt, double mu, double sigma, double S0, double strike,
                 vector2d &S) {
    mcAnalyticalc(startPath, numPaths,numSteps,  T,  dt,
                     sqrt_dt,  mu,  sigma,  S0,
                  S);
    output = payoff(startPath, numPaths, S, strike);
    }

  int main() {

    <<blInputs>>
      
    vector2d S(numPaths, std::vector<double>(numSteps, 0.));
    unsigned int numThreads = std::thread::hardware_concurrency();
    std::vector<double> partialSums(numThreads, 0.);
    std::vector<std::thread> threads;
    unsigned long pathsPerThread = numPaths / numThreads;
    auto start = std::chrono::high_resolution_clock::now();
    // Launch threads
    for (unsigned int i = 0; i < numThreads; ++i) {
      unsigned long startPath = i * pathsPerThread;
      if (i == numThreads - 1) pathsPerThread = numPaths - startPath;
      threads.emplace_back(fthread, std::ref(partialSums[i]), startPath, pathsPerThread, numSteps, T,
                           dt, sqrt_dt, rate, vol, spot, strike, std::ref(S));
    }

    // Join threads
    for (auto &t : threads) {
      t.join();
    }
    double totalSum = std::accumulate(partialSums.begin(), partialSums.end(), 0.);
    double price = totalSum / numThreads;
    auto end = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> duration = end - start;

    // std::cout << "Estimated average final value: " << averageFinalValue <<
    // std::endl;
    std::cout << "Totalpaths: " << numPaths << std::endl;
    std::cout << "Duration(seconds): " << duration.count() << std::endl;
    std::cout << "Price: " << price << std::endl;

    return 0;
  }

#+end_src

#+RESULTS:
| Totalpaths:        | 5000000 |
| Duration(seconds): | 33.5171 |
| Price:             | 10.6291 |

* MPI
#+begin_src C++
  #include <mpi.h>
  #include <iostream>
  #include <vector>
  #include <random>
  #include <cmath>

  // Function to perform Euler-Maruyama simulation for a single path
  double simulatePath(double x0, double mu, double sigma, double T, int numSteps, std::mt19937& rng) {
      std::normal_distribution<double> normal_dist(0.0, 1.0);
      double dt = T / numSteps;
      double x = x0;

      for (int i = 0; i < numSteps; ++i) {
          double dW = normal_dist(rng) * std::sqrt(dt); // Wiener process increment
          x += mu * dt + sigma * dW;                   // Euler-Maruyama update
      }
      return x;
  }

  int main(int argc, char** argv) {
      MPI_Init(&argc, &argv);

      int rank, size;
      MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get process rank
      MPI_Comm_size(MPI_COMM_WORLD, &size); // Get number of processes

      // Parameters for the SDE
      double x0 = 1.0;         // Initial value
      double mu = 0.1;         // Drift coefficient
      double sigma = 0.2;      // Volatility coefficient
      double T = 1.0;          // Total time
      int numSteps = 1000;     // Number of time steps
      int numPaths = 1000000;  // Total number of paths to simulate

      // Divide paths among processes
      int pathsPerProcess = numPaths / size;
      if (rank == 0 && numPaths % size != 0) {
          std::cerr << "Warning: numPaths is not divisible by numProcesses; some paths may be skipped." << std::endl;
      }

      // Seed the random number generator uniquely for each process
      std::random_device rd;
      std::mt19937 rng(rd() + rank);

      // Each process simulates its share of paths
      std::vector<double> localResults(pathsPerProcess);
      for (int i = 0; i < pathsPerProcess; ++i) {
          localResults[i] = simulatePath(x0, mu, sigma, T, numSteps, rng);
      }

      // Compute local mean
      double localSum = 0.0;
      for (double result : localResults) {
          localSum += result;
      }
      double localMean = localSum / pathsPerProcess;

      // Gather results to rank 0
      double globalSum = 0.0;
      MPI_Reduce(&localMean, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

      // Rank 0 computes and prints the global mean
      if (rank == 0) {
          double globalMean = globalSum / size;
          std::cout << "Global mean of final values: " << globalMean << std::endl;
      }

      MPI_Finalize();
      return 0;
  }
#+end_src

* OpenMP
#+begin_src C++
  #include <iostream>
  #include <vector>
  #include <random>
  #include <cmath>
  #include <omp.h>  // OpenMP header

  // Function to perform Euler-Maruyama simulation for a single path
  double simulatePath(double x0, double mu, double sigma, double T, int numSteps, std::mt19937& rng) {
      std::normal_distribution<double> normal_dist(0.0, 1.0);
      double dt = T / numSteps; // Time step size
      double x = x0;

      for (int i = 0; i < numSteps; ++i) {
          double dW = normal_dist(rng) * std::sqrt(dt); // Wiener process increment
          x += mu * dt + sigma * dW;                   // Euler-Maruyama update
      }

      return x;
  }

  int main() {
      // Parameters for the SDE
      double x0 = 1.0;         // Initial value
      double mu = 0.1;         // Drift coefficient
      double sigma = 0.2;      // Volatility coefficient
      double T = 1.0;          // Total simulation time
      int numSteps = 1000;     // Number of time steps
      int numPaths = 1000000;  // Number of paths for the Monte Carlo simulation

      // Array to store results
      std::vector<double> results(numPaths);

      // Start parallel region
      #pragma omp parallel
      {
          // Each thread gets its own random number generator
          std::random_device rd;
          std::mt19937 rng(rd() + omp_get_thread_num()); // Seed RNG uniquely for each thread

          // Parallel loop for Monte Carlo simulation
          #pragma omp for
          for (int i = 0; i < numPaths; ++i) {
              results[i] = simulatePath(x0, mu, sigma, T, numSteps, rng);
          }
      }

      // Compute mean of all paths
      double totalSum = 0.0;
      #pragma omp parallel for reduction(+:totalSum)
      for (int i = 0; i < numPaths; ++i) {
          totalSum += results[i];
      }
      double mean = totalSum / numPaths;

      // Output the mean
      std::cout << "Mean of Monte Carlo simulation: " << mean << std::endl;

      return 0;
  }
#+end_src



https://www.codeproject.com/Articles/813485/A-High-Performance-Monte-Carlo-Integration-Simulat
