* Cuda

#+begin_src C++
  #include <iostream>
  #include <curand_kernel.h>

  __global__ void monteCarloSDE(float* d_samples, int numPaths, int numSteps, float dt, float mu, float sigma, float x0) {
      int pathIdx = blockIdx.x * blockDim.x + threadIdx.x;
      if (pathIdx >= numPaths) return;

      // Initialize the state of the random number generator for this thread
      curandState state;
      curand_init(1234, pathIdx, 0, &state);

      // Simulate the SDE for this path
      float x = x0;
      for (int i = 0; i < numSteps; ++i) {
          float dW = curand_normal(&state) * sqrtf(dt); // Wiener increment
          x += mu * dt + sigma * dW; // Euler-Maruyama update
      }

      // Store the final value for this path
      d_samples[pathIdx] = x;
  }

  int main() {
      // Parameters
      int numPaths = 1 << 16;  // Number of Monte Carlo paths
      int numSteps = 1000;     // Time steps per path
      float T = 1.0f;          // Total time
      float dt = T / numSteps; // Time step size
      float mu = 0.1f;         // Drift coefficient
      float sigma = 0.2f;      // Volatility coefficient
      float x0 = 1.0f;         // Initial value

      // Memory allocation (host and device)
      float* h_samples = new float[numPaths];
      float* d_samples;
      cudaMalloc(&d_samples, numPaths * sizeof(float));

      // Define CUDA kernel configuration
      int threadsPerBlock = 256;
      int numBlocks = (numPaths + threadsPerBlock - 1) / threadsPerBlock;

      // Launch kernel
      monteCarloSDE<<<numBlocks, threadsPerBlock>>>(d_samples, numPaths, numSteps, dt, mu, sigma, x0);

      // Copy results back to host
      cudaMemcpy(h_samples, d_samples, numPaths * sizeof(float), cudaMemcpyDeviceToHost);

      // Compute the mean of the simulated final values
      float mean = 0.0f;
      for (int i = 0; i < numPaths; ++i) {
          mean += h_samples[i];
      }
      mean /= numPaths;

      // Output results
      std::cout << "Mean of the final values: " << mean << std::endl;

      // Clean up
      delete[] h_samples;
      cudaFree(d_samples);

      return 0;
  }
  
#+end_src

** Multiple device

#+begin_src C++
  #include <iostream>
  #include <cuda_runtime.h>
  #include <curand_kernel.h>

  // CUDA kernel to simulate SDE paths using Euler-Maruyama
  __global__ void monteCarloSDE(float* d_results, int numPaths, int numSteps, float dt, float mu, float sigma, float x0, unsigned long seed) {
      int tid = blockIdx.x * blockDim.x + threadIdx.x;
      if (tid >= numPaths) return;

      // Initialize random number generator state
      curandState state;
      curand_init(seed, tid, 0, &state);

      // Simulate SDE using Euler-Maruyama
      float x = x0;
      for (int i = 0; i < numSteps; ++i) {
          float dW = curand_normal(&state) * sqrtf(dt); // Wiener process increment
          x += mu * dt + sigma * dW;                   // Euler-Maruyama update
      }

      // Store the final value for this path
      d_results[tid] = x;
  }

  int main() {
      // Problem configuration
      int numPaths = 1000000;  // Total number of Monte Carlo paths
      int numSteps = 1000;     // Number of time steps
      float dt = 0.001f;       // Time step size
      float mu = 0.1f;         // Drift coefficient
      float sigma = 0.2f;      // Volatility coefficient
      float x0 = 1.0f;         // Initial value

      // Query number of available GPUs
      int numDevices;
      cudaGetDeviceCount(&numDevices);
      if (numDevices < 1) {
          std::cerr << "No CUDA-capable devices found." << std::endl;
          return 1;
      }
      std::cout << "Number of CUDA devices: " << numDevices << std::endl;

      // Divide paths across devices
      int pathsPerDevice = numPaths / numDevices;

      // Results on host
      float* h_results = new float[numPaths];

      // Per-device resources
      float* d_results[numDevices];
      cudaStream_t streams[numDevices];
      for (int i = 0; i < numDevices; ++i) {
          // Set device
          cudaSetDevice(i);

          // Allocate memory for results on GPU
          cudaMalloc(&d_results[i], pathsPerDevice * sizeof(float));

          // Create a CUDA stream for each device
          cudaStreamCreate(&streams[i]);
      }

      // Launch kernels on each device
      for (int i = 0; i < numDevices; ++i) {
          cudaSetDevice(i);

          // Calculate grid and block size
          int threadsPerBlock = 256;
          int numBlocks = (pathsPerDevice + threadsPerBlock - 1) / threadsPerBlock;

          // Launch kernel
          monteCarloSDE<<<numBlocks, threadsPerBlock, 0, streams[i]>>>(
              d_results[i], pathsPerDevice, numSteps, dt, mu, sigma, x0, 1234 + i);

          // Check for kernel errors
          cudaError_t err = cudaGetLastError();
          if (err != cudaSuccess) {
              std::cerr << "CUDA kernel error on device " << i << ": " << cudaGetErrorString(err) << std::endl;
              return 1;
          }
      }

      // Copy results back to host
      for (int i = 0; i < numDevices; ++i) {
          cudaSetDevice(i);
          cudaMemcpyAsync(h_results + i * pathsPerDevice, d_results[i],
                          pathsPerDevice * sizeof(float), cudaMemcpyDeviceToHost, streams[i]);
      }

      // Synchronize all streams
      for (int i = 0; i < numDevices; ++i) {
          cudaSetDevice(i);
          cudaStreamSynchronize(streams[i]);
      }

      // Compute global mean
      double globalSum = 0.0;
      for (int i = 0; i < numPaths; ++i) {
          globalSum += h_results[i];
      }
      double globalMean = globalSum / numPaths;

      // Print the result
      std::cout << "Global mean of Monte Carlo simulation: " << globalMean << std::endl;

      // Clean up
      for (int i = 0; i < numDevices; ++i) {
          cudaSetDevice(i);
          cudaFree(d_results[i]);
          cudaStreamDestroy(streams[i]);
      }
      delete[] h_results;

      return 0;
  }
#+end_src
* MPI
#+begin_src C++
  #include <mpi.h>
  #include <iostream>
  #include <vector>
  #include <random>
  #include <cmath>

  // Function to perform Euler-Maruyama simulation for a single path
  double simulatePath(double x0, double mu, double sigma, double T, int numSteps, std::mt19937& rng) {
      std::normal_distribution<double> normal_dist(0.0, 1.0);
      double dt = T / numSteps;
      double x = x0;

      for (int i = 0; i < numSteps; ++i) {
          double dW = normal_dist(rng) * std::sqrt(dt); // Wiener process increment
          x += mu * dt + sigma * dW;                   // Euler-Maruyama update
      }
      return x;
  }

  int main(int argc, char** argv) {
      MPI_Init(&argc, &argv);

      int rank, size;
      MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get process rank
      MPI_Comm_size(MPI_COMM_WORLD, &size); // Get number of processes

      // Parameters for the SDE
      double x0 = 1.0;         // Initial value
      double mu = 0.1;         // Drift coefficient
      double sigma = 0.2;      // Volatility coefficient
      double T = 1.0;          // Total time
      int numSteps = 1000;     // Number of time steps
      int numPaths = 1000000;  // Total number of paths to simulate

      // Divide paths among processes
      int pathsPerProcess = numPaths / size;
      if (rank == 0 && numPaths % size != 0) {
          std::cerr << "Warning: numPaths is not divisible by numProcesses; some paths may be skipped." << std::endl;
      }

      // Seed the random number generator uniquely for each process
      std::random_device rd;
      std::mt19937 rng(rd() + rank);

      // Each process simulates its share of paths
      std::vector<double> localResults(pathsPerProcess);
      for (int i = 0; i < pathsPerProcess; ++i) {
          localResults[i] = simulatePath(x0, mu, sigma, T, numSteps, rng);
      }

      // Compute local mean
      double localSum = 0.0;
      for (double result : localResults) {
          localSum += result;
      }
      double localMean = localSum / pathsPerProcess;

      // Gather results to rank 0
      double globalSum = 0.0;
      MPI_Reduce(&localMean, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

      // Rank 0 computes and prints the global mean
      if (rank == 0) {
          double globalMean = globalSum / size;
          std::cout << "Global mean of final values: " << globalMean << std::endl;
      }

      MPI_Finalize();
      return 0;
  }
#+end_src

* OpenMP
#+begin_src C++
  #include <iostream>
  #include <vector>
  #include <random>
  #include <cmath>
  #include <omp.h>  // OpenMP header

  // Function to perform Euler-Maruyama simulation for a single path
  double simulatePath(double x0, double mu, double sigma, double T, int numSteps, std::mt19937& rng) {
      std::normal_distribution<double> normal_dist(0.0, 1.0);
      double dt = T / numSteps; // Time step size
      double x = x0;

      for (int i = 0; i < numSteps; ++i) {
          double dW = normal_dist(rng) * std::sqrt(dt); // Wiener process increment
          x += mu * dt + sigma * dW;                   // Euler-Maruyama update
      }

      return x;
  }

  int main() {
      // Parameters for the SDE
      double x0 = 1.0;         // Initial value
      double mu = 0.1;         // Drift coefficient
      double sigma = 0.2;      // Volatility coefficient
      double T = 1.0;          // Total simulation time
      int numSteps = 1000;     // Number of time steps
      int numPaths = 1000000;  // Number of paths for the Monte Carlo simulation

      // Array to store results
      std::vector<double> results(numPaths);

      // Start parallel region
      #pragma omp parallel
      {
          // Each thread gets its own random number generator
          std::random_device rd;
          std::mt19937 rng(rd() + omp_get_thread_num()); // Seed RNG uniquely for each thread

          // Parallel loop for Monte Carlo simulation
          #pragma omp for
          for (int i = 0; i < numPaths; ++i) {
              results[i] = simulatePath(x0, mu, sigma, T, numSteps, rng);
          }
      }

      // Compute mean of all paths
      double totalSum = 0.0;
      #pragma omp parallel for reduction(+:totalSum)
      for (int i = 0; i < numPaths; ++i) {
          totalSum += results[i];
      }
      double mean = totalSum / numPaths;

      // Output the mean
      std::cout << "Mean of Monte Carlo simulation: " << mean << std::endl;

      return 0;
  }
#+end_src



https://www.codeproject.com/Articles/813485/A-High-Performance-Monte-Carlo-Integration-Simulat
